{
 "metadata": {
  "gist_id": "3eebeb7824408d377522",
  "name": "",
  "signature": "sha256:a1a7821a3184c4b31ca84cbb56bc640c51f1415bd405e55203e1b493a90cbb60"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Fitting a simple model to data using MCMC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An example of using MCMC to fit a straight line to data points, possibly with known individual measurement errors, while accounting for intrinsic scatter and outliers.  The method is easily generalisable to more complicated models and datasets.\n",
      "\n",
      "We make use of the awesome `emcee` sampler by Dan Foreman-Mackey and collaborators.  Normal `emcee` is contrasted with the parallel-tempered version, and they are shown to give identical results, especially if there is some very light cleaning applied to the normal chains.\n",
      "\n",
      "This notebook is based on the examples and tutorials provided on [the emcee webpages](http://dan.iel.fm/emcee/), Jake Vanderplas' [wonderful series of blog posts](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/), and David Hogg, Jo Bovy and Dustin Lang's insightful [paper on the subject](http://arxiv.org/abs/1008.4686)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from matplotlib import pyplot as plt\n",
      "import numpy as np\n",
      "import emcee\n",
      "import triangle\n",
      "from math import floor\n",
      "\n",
      "# better-looking plots\n",
      "plt.rcParams['font.family'] = 'serif'\n",
      "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
      "plt.rcParams['font.size'] = 12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define some helper functions\n",
      "\n",
      "def flatten_without_burn(sampler, nburn):\n",
      "    c = sampler.chain\n",
      "    if c.ndim == 4:\n",
      "        c = c[0]\n",
      "    c = c[:, nburn:]\n",
      "    return c.reshape((np.product(c.shape[:-1]), c.shape[-1]))\n",
      "\n",
      "\n",
      "def weight_without_burn(sampler, nburn):\n",
      "    c = sampler.lnprobability\n",
      "    if c.ndim == 3:\n",
      "        c = c[0]\n",
      "    c = c[:, nburn:]\n",
      "    w = np.exp(c.reshape(np.product(c.shape)))\n",
      "    return w / w.sum()\n",
      "\n",
      "\n",
      "def get_samples(sampler, nburn, minweight=None):\n",
      "    sample = flatten_without_burn(sampler, nburn)\n",
      "    if minweight is not None:\n",
      "        weight = weight_without_burn(sampler, nburn)\n",
      "        sample = sample[weight > minweight]\n",
      "    return sample\n",
      "\n",
      "\n",
      "def minmaxpad(x, p=0.05):\n",
      "    xmin = x.min()\n",
      "    xmax = x.max()\n",
      "    xrange = xmax - xmin\n",
      "    xmin = xmin - p*xrange\n",
      "    xmax = xmax + p*xrange\n",
      "    return xmin, xmax\n",
      "\n",
      "\n",
      "def plot_MCMC_model(ax, xdata, ydata, trace):\n",
      "    \"\"\"Plot the linear model and 2sigma contours\"\"\"\n",
      "    ax.plot(xdata, ydata, 'ow')\n",
      "\n",
      "    xmin, xmax = minmaxpad(x)\n",
      "    dx = (xmax-xmin)/100.0\n",
      "    xfit = np.linspace(xmin, xmax, 100)\n",
      "    yfit = model(xfit[...,np.newaxis], trace.T)\n",
      "    mu = yfit.mean(-1)\n",
      "    sig = yfit.std(-1)\n",
      "    \n",
      "    ax.fill_between(xfit, mu - 2*sig, mu + 2*sig, color='lightgray')\n",
      "    ax.fill_between(xfit, mu - sig, mu + sig, color='darkgray')\n",
      "    ax.plot(xfit, mu, '-k')\n",
      "\n",
      "    ax.set_xlabel('x')\n",
      "    ax.set_ylabel('y')\n",
      "    ax.set_xlim([xmin, xmax])\n",
      "\n",
      "    \n",
      "def round_sig(x, sig=1):\n",
      "    d = sig-int(floor(log10(x)))-1\n",
      "    d = max(0, d)\n",
      "    return round(x, d), d\n",
      "\n",
      "\n",
      "def summary(samples, truths=None):\n",
      "    mean = samples.mean(0)\n",
      "    sigma = samples.std(0)\n",
      "    for i, p in enumerate(par):\n",
      "        err, dp = round_sig(sigma[i], 1)\n",
      "        val = round(mean[i], dp)\n",
      "        dp = str(dp)\n",
      "        dp += 'e}' if abs(log10(val)) > 3 else 'f}'\n",
      "        outstr = ('{:16s} = {:8.'+dp+' \u00b1 {:<8.'+dp).format(p, val, err)\n",
      "        if truths is not None:\n",
      "            outstr += ('   ('+dp+')').format(truths[i])\n",
      "        print(outstr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# same data as last week\n",
      "x = np.array([ 1,  3,  9, 14, 15, 19, 20, 21, 30, 35,\n",
      "              40, 41, 42, 47, 54, 56, 67, 69, 72, 88])\n",
      "\n",
      "y = np.array([33, 68, 34, 34, 37, 65, 37, 44, 48, 49,\n",
      "              53, 49, 50, 48, 56, 60, 61, 63, 44, 71])\n",
      "\n",
      "e = np.array([3.6, 3.9, 2.6, 3.4, 3.8, 14.8, 2.2, 2.1, 2.3, 3.8,\n",
      "              2.2, 2.8, 3.9, 3.1, 3.4, 2.6, 3.4, 3.7, 2.0, 3.5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "meanx = x.mean()\n",
      "print 'meanx = {}'.format(meanx)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# check the data\n",
      "xmin, xmax = minmaxpad(x)\n",
      "plt.errorbar(x, y, e, fmt='ok', alpha=0.2)\n",
      "_ = plt.plot([xmin, xmax], [xmin, xmax], lw=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define all aspects of the model\n",
      "\n",
      "xcentre = 14.2\n",
      "\n",
      "def model(x, theta):\n",
      "    intercept, slope, scatter, prob_outlier = theta\n",
      "    return intercept + slope * (x - xcentre)\n",
      "\n",
      "def log_prior(theta):\n",
      "    intercept, slope, scatter, prob_outlier = theta\n",
      "    # scatter must be greater than zero\n",
      "    if scatter <= 0:\n",
      "        return -np.inf\n",
      "    # prob_outlier must be between zero and one\n",
      "    if prob_outlier < 0 or prob_outlier > 1:\n",
      "        return -np.inf\n",
      "    # prior on intercept, slope and scatter; as explained at\n",
      "    # http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/#The-Prior\n",
      "    # P(intercept) = 1\n",
      "    # P(slope) = (1 + slope ** 2)^(-1.5)  # uniform in sin(angle)\n",
      "    # P(scatter) = 1/scatter  # Jeffreys prior, invariant to rescaling\n",
      "    return -1.5 * np.log(1 + slope ** 2) - np.log(scatter)\n",
      "\n",
      "# scatter of outlier points, just needs to be much larger than normal scatter\n",
      "scatter_outliers = 50.0\n",
      "\n",
      "def log_likelihood(theta, x, y, e, scatter_outliers=scatter_outliers):\n",
      "    intercept, slope, scatter, prob_outlier = theta\n",
      "    # square residuals\n",
      "    y_model = model(x, theta)\n",
      "    dy2 = (y - y_model)**2\n",
      "    # avoid NaNs in logarithm\n",
      "    prob_outlier = np.clip(prob_outlier, 1e-99, 1-1e-99)\n",
      "    # compute effective variances by combining errors and intrinsic scatter\n",
      "    eff_var = scatter**2 + e**2\n",
      "    eff_var_outliers = scatter_outliers**2 + e**2\n",
      "    # logL for good (normal) and bad (outlier) distributions\n",
      "    logL_good = (np.log(1 - prob_outlier)\n",
      "                 - 0.5 * np.log(2 * np.pi * eff_var)\n",
      "                 - 0.5 * dy2 / eff_var)\n",
      "    logL_bad = (np.log(prob_outlier)\n",
      "                - 0.5 * np.log(2 * np.pi * eff_var_outliers)\n",
      "                - 0.5 * dy2 / eff_var_outliers)    \n",
      "    # using np.logaddexp helps maintain numerical precision\n",
      "    return np.sum(np.logaddexp(logL_good, logL_bad))\n",
      "\n",
      "def logl(theta):\n",
      "    # PT sampler needs us to use global variables\n",
      "    return log_likelihood(theta, x, y, e, scatter_outliers)\n",
      "\n",
      "def log_posterior(theta, x, y, e, scatter_outliers=scatter_outliers):\n",
      "    return log_prior(theta) + log_likelihood(theta, x, y, e, scatter_outliers)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set up emcee\n",
      "np.random.seed(666)  # reproducible\n",
      "\n",
      "par = ('intercept', 'slope', 'scatter', 'prob_outlier')\n",
      "ndim = len(par)  # number of parameters in the model\n",
      "nwalkers = 50  # number of MCMC walkers\n",
      "nburn = 500  # \"burn-in\" period to let chains stabilize\n",
      "nsamp = 500  # number of MCMC steps to take after burn-in\n",
      "\n",
      "initial_theta = np.zeros((nwalkers, ndim))\n",
      "initial_theta[:, :2] = np.random.normal((14, 1.0), 1, (nwalkers, 2))\n",
      "initial_theta[:, 2:] = np.random.uniform((0.1, 0.1), (1.0, 0.5), (nwalkers, 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# perform sampling\n",
      "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=[x, y, e], threads=4)\n",
      "r = sampler.run_mcmc(initial_theta, nburn+nsamp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# check autocorrelation time is many times larger than nburn and nsamp\n",
      "a_exp = sampler.acor\n",
      "print a_exp\n",
      "a_int = np.max([emcee.autocorr.integrated_time(sampler.chain[i]) for i in range(len(sampler.chain))], 0)\n",
      "print a_int\n",
      "a_exp = max(a_exp)\n",
      "a_int = max(a_int)\n",
      "print('A reasonable burn-in should be around {:d} steps'.format(int(10*a_exp)))\n",
      "print('After burn-in, each chain produces one independent sample per {:d} steps'.format(int(a_int)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# perform parallel-tempered sampling\n",
      "ntemps = 5  # number of temperature rungs\n",
      "sampler_pt = emcee.PTSampler(ntemps, nwalkers, ndim, logl, log_prior, threads=4)\n",
      "r_pt = sampler_pt.run_mcmc(initial_theta*np.ones((ntemps, nwalkers, ndim)), nburn+nsamp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# check autocorrelation time is many times larger than nburn and nsamp\n",
      "a_exp = sampler_pt.acor[0]\n",
      "print a_exp\n",
      "a_int = np.max([emcee.autocorr.integrated_time(sampler_pt.chain[0, i, nburn:]) for i in range(len(sampler_pt.chain[0]))], 0)\n",
      "print a_int\n",
      "a_exp = max(a_exp)\n",
      "a_int = max(a_int)\n",
      "print('A reasonable burn-in should be around {:d} steps'.format(int(10*a_exp)))\n",
      "print('After burn-in, each chain produces one independent sample per {:d} steps'.format(int(a_int)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot the chains to visually assess convergence\n",
      "plt.figure(figsize=[20,10])\n",
      "for i, p in enumerate(par):\n",
      "    plt.subplot(2,2,i+1)\n",
      "    for w in range(nwalkers):\n",
      "        plt.plot(numpy.arange(sampler.chain.shape[1]), sampler.chain[w,:,i], 'b-', alpha=0.1)\n",
      "    for w in range(nwalkers):\n",
      "            plt.plot(numpy.arange(sampler_pt.chain.shape[2]), sampler_pt.chain[0,w,:,i], 'g-', alpha=0.1)\n",
      "    plt.ylabel(p)\n",
      "    aymin, aymax = plt.ylim()\n",
      "    plt.vlines(nburn, aymin, aymax, linestyle=':')\n",
      "    plt.ylim(aymin, aymax)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot the chains to visually assess auto correlation time at equilibrium\n",
      "plt.figure(figsize=[20,10])\n",
      "for i, p in enumerate(par):\n",
      "    plt.subplot(2,2,i+1)\n",
      "    for w in range(0,nwalkers,10):\n",
      "        plt.plot(numpy.arange(100), sampler.chain[w,nburn:nburn+100,i], 'b-')\n",
      "    for w in range(0,nwalkers,10):\n",
      "            plt.plot(numpy.arange(100), sampler_pt.chain[0,w,nburn:nburn+100,i], 'g-')\n",
      "    plt.ylabel(p)\n",
      "    aymin = np.min(sampler_pt.chain[0,:,nburn:,i])\n",
      "    aymax = np.max(sampler_pt.chain[0,:,nburn:nburn+100,i])\n",
      "    plt.ylim(aymin, aymax)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert chains into samples for each parameter\n",
      "# clip chains with very low weights for normal method\n",
      "samples = get_samples(sampler, nburn, minweight=1e-15)\n",
      "samples_full = get_samples(sampler, nburn)\n",
      "samples_pt = get_samples(sampler_pt, nburn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# examine parameter histograms and compare normal and parallel methods\n",
      "plt.figure(figsize=[20,10])\n",
      "for i, p in enumerate(par):\n",
      "    plt.subplot(2,(ndim+1)//2,i+1)\n",
      "    n, b, patches = plt.hist(samples_full[:,i], bins=100, color='b', histtype='stepfilled', log=True)\n",
      "    plt.hist(samples[:,i], bins=b, color='r', histtype='stepfilled', alpha=0.5)\n",
      "    plt.hist(samples_pt[:,i], bins=b, color='g', histtype='step', lw=2)\n",
      "    plt.xlabel(p)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create mega plot for normal method\n",
      "triangle.corner(samples_full, labels=par)\n",
      "ax = plt.subplot(2, 2, 2)\n",
      "plot_MCMC_model(ax, x, y, samples)\n",
      "plt.plot([xmin, xmax], [xmin, xmax], lw=2)\n",
      "plt.subplots_adjust(wspace=0.15, hspace=0.15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create mega plot for parallel-tempered method\n",
      "triangle.corner(samples_pt, labels=par)\n",
      "ax = plt.subplot(2, 2, 2)\n",
      "plot_MCMC_model(ax, x, y, samples_pt)\n",
      "plt.plot([xmin, xmax], [xmin, xmax], lw=2)\n",
      "plt.subplots_adjust(wspace=0.15, hspace=0.15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output a summary of the normal results\n",
      "summary(samples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output a summary of the parallel-tempered results\n",
      "summary(samples_pt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}